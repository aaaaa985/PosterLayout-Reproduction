<div align="center">
<h1 align="center">PosterLayout Reproduction (PyTorch & Jittor)</h1>

<p align="center">
  <b>Nankai University | Sprout Program (æ–°èŠ½è®¡åˆ’)</b>
</p>

<p align="center">
  <a href="https://github.com/Jittor/jittor"><img src="https://img.shields.io/badge/Framework-Jittor-red.svg" alt="Jittor"></a>
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/Framework-PyTorch-orange.svg" alt="PyTorch"></a>
  <a href="./LICENSE"><img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License"></a>
  <a href="https://github.com/aaaaa985/PosterLayout-Reproduction"><img src="https://img.shields.io/github/stars/aaaaa985/PosterLayout-Reproduction?style=social" alt="Stars"></a>
</p>

<p align="center">
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_PosterLayout_A_New_Benchmark_and_Approach_for_Content-Aware_Automatic_Poster_CVPR_2023_paper.pdf"><img src="https://img.shields.io/badge/Paper-CVPR%202023-brightgreen" alt="Paper"></a>
  <a href="https://github.com/PKU-Venzu/PosterLayout"><img src="https://img.shields.io/badge/Official-Repo-lightgrey?logo=github" alt="Official Repo"></a>
</p>
---

## English

### ğŸš€Introduction

This repository contains the reproduction and framework migration of the paper **"PosterLayout: A New Benchmark and Approach for Content-aware Automatic Poster Layout"**. 

This project is the final result of the **"AI Practice Course (Elementary)"** at **Nankai University** and belongs to the **"Sprout Program" (æ–°èŠ½è®¡åˆ’)** of Nankai University. The work includes:
1. A complete reproduction of the original model using **PyTorch**.
2. A successful migration to the **Jittor (è®¡å›¾)** framework, a high-performance deep learning framework developed by Tsinghua University.

### ğŸ—ï¸Model Overview (PosterLayout / DS-GAN)

The PosterLayout model, also known as **DS-GAN** (Design Sequence GAN), is a content-aware poster layout generation framework. 
- **Generator**: Takes a background image and a saliency map as input. A visual backbone extracts contextual features ($h_0$), while a 1D-CNN and a BiLSTM reason over the design sequence to predict element categories ($c_i$) and bounding boxes ($b_i$).
- **Discriminator**: Evaluates the compatibility between the generated layout and the background.
- **Optimization**: The model is supervised by a combination of reconstruction loss ($L_{rec}$: NLL + L1 + GIoU) and adversarial loss ($L_{adv}$: Hinge loss).

<p align="center">
  <img src="./assets/framework.png" width="100%" />
  <br>
  <em>Figure: Overview of the DS-GAN architecture.</em>
</p>

### ğŸ’¡Migration Highlights (Technical Details)

During the migration from PyTorch to Jittor, the following optimizations were implemented based on framework characteristics:

- **Custom LSTM Layer**: Manually implemented the bidirectional LSTM structure in Jittor to ensure seamless loading of pre-trained weights.
- **Pooling Operator Adaptation**: Addressed Jittor's strict input size validation for pooling layers by using a "Wide-Concat" strategy to bypass dimension constraints.
- **Memory Synchronization**: Utilized jt.sync in the discriminator logic to precisely control synchronization, solving data consistency issues with the ArgMax operator in GPU environments.
- **Cross-Framework Alignment**: Designed loss mapping to ensure the Jittor version achieves parity with the PyTorch version in convergence speed and evaluation metrics.

### ğŸ¨Results Showcase

I provide a comprehensive evaluation of the model performance, including visual samples, quantitative metrics (Triple-line Table format), and training convergence plots for both frameworks. The model demonstrates a strong ability to generate content-aware layouts that respect the visual hierarchy and saliency of the background images.

#### Visual Comparison

<p align="center">
  <img src="./assets/pytorch_res1.png" width="45%" alt="PyTorch Sample 1"/>
  <img src="./assets/pytorch_res2.png" width="45%" alt="PyTorch Sample 2"/>
  <br><em>PyTorch Implementation Results</em>
</p>

<p align="center">
  <img src="./assets/jittor_res1.png" width="45%" alt="Jittor Sample 1"/>
  <img src="./assets/jittor_res2.png" width="45%" alt="Jittor Sample 2"/>
  <br><em>Jittor Implementation Results</em>
</p>

#### Quantitative Results

The following table summarizes the performance at **Epoch 300**. (â†‘) indicates higher is better, (â†“) indicates lower is better. 

<table>   <thead>     <tr>       <th>Method / Framework</th>       <th>Epoch</th>       <th>Val(â†‘)</th>       <th>Ove(â†“)</th>       <th>Ali(â†“)</th>       <th>Und_L(â†‘)</th>       <th>Und_S(â†‘)</th>       <th>Uti(â†‘)</th>       <th>Occ(â†“)</th>       <th>Rea(â†“)</th>     </tr>   </thead>   <tbody>     <tr>       <td><b>(Ours) PyTorch</b></td>       <td>300</td>       <td>1.0000</td>       <td>0.0137</td>       <td><b>0.0039</b></td>       <td><b>0.4184</b></td>       <td>0.2082</td>       <td><b>0.9786</b></td>       <td>0.3468</td>       <td><b>0.0121</b></td>     </tr>     <tr>       <td><b>(Ours) Jittor</b></td>       <td>300</td>       <td>1.0000</td>       <td><b>0.0076</b></td>       <td>0.0045</td>       <td>0.2380</td>       <td><b>0.4999</b></td>       <td>0.7371</td>       <td><b>0.1763</b></td>       <td>0.1460</td>     </tr>   </tbody> </table>

#### Training Convergence

To demonstrate the stability of our reproduction, we provide the metric trends recorded during the training process for both PyTorch and Jittor versions.

<p align="center">   <img src="./assets/pytorch_metrics.png" width="80%" />   <br><em>Metrics Trend: PyTorch Version</em> </p>

<p align="center">   <img src="./assets/jittor_metrics.png" width="80%" />   <br><em>Metrics Trend: Jittor Version</em> </p>

### ğŸ”Result Analysis

- **Framework Consistency**: Both frameworks achieved a **Validity of 1.0**, indicating that the generated layouts are fully compliant with basic geometric constraints.
- **Metric Highlights**:
  - The **PyTorch** version excelled in **Utility (0.9786)** and **Readability (0.0121)**, showing superior performance in avoiding visual centers and maintaining text clarity.
  - The **Jittor** version demonstrated better performance in **Overlap (0.0076)** and **Underlay_S (0.4999)**, suggesting that Jittor implementation might be more effective in controlling element occlusion and maintaining underlay containment.

- **Convergence**: As shown in the trend plots, both implementations show stable convergence across all 300 epochs, validating the effectiveness of our hyperparameters and migration logic.

### ğŸ“–Usage

1. **Data Preparation**: Follow the instructions in [data/README.md](https://www.google.com/url?sa=E&q=./data/README.md) to download the **PosterLLaVa** dataset. Ensure the images and JSON files are placed in the `data/` directory.

2. **Download Weights**: Download the pretrained `.pth` (PyTorch) or `.jtp` (Jittor) weights from [Releases](https://www.google.com/url?sa=E&q=../../releases) and place them into the `model_weight/` folder.

3. **Training & Inference**:
   Navigate to the desired framework directory:

   ```
   cd jittor_version  # or cd pytorch_version
   python main.py     # For training
   python infer.py    # For batch inference
   python eval.py     # For metrics calculation and visualization
   ```

### ğŸ› ï¸Installation & Setup

#### Pytorch Version

**Install Python Dependencies**:

```
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

#### Jittor Version

1. **Install Python Dependencies**:

   ```
   python -m pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
   ```

2. **Initialize Jittor CUDA Environment** (For GPU servers like AutoDL):
   Jittor requires a C++ compilation environment. Use the following command to let Jittor automatically download and configure the built-in CUDA and cuDNN:

   ```
   python -m jittor_utils.install_cuda
   ```

3. **Fix System Paths** (Handling `libGL` or missing headers):
   If using older disk images or encountering `cudnn.h not found`, execute:

   ```
   # 1. Try creating a symbolic link (requires sudo)
   sudo ln -s /usr/include/cudnn.h /usr/local/cuda/include/cudnn.h 2>/dev/null
   
   # 2. If sudo is not available, configure environment variables
   export C_INCLUDE_PATH=$C_INCLUDE_PATH:/usr/include
   export CPLUS_INCLUDE_PATH=$CPLUS_INCLUDE_PATH:/usr/include
   # Resolve conflicts between OpenMP and MKL
   export KMP_DUPLICATE_LIB_OK=TRUE
   export MKL_SERVICE_FORCE_INTEL=1
   ```

### ğŸ““Technical Notes

#### Pytorch Version

- **Backbone Library**: The `timm` (PyTorch Image Models) library is required to initialize the ResNet backbones. The implementation supports loading both local weights and official pre-trained models.
- **Hungarian Matcher**: Similar to the original DETR architecture, we utilize `scipy.optimize.linear_sum_assignment` in `RecLoss.py` to solve the bipartite matching problem between predicted and ground-truth boxes.
- **Geometric Operations**: Advanced bounding box operations (such as `box_area`) are handled via `torchvision.ops` to ensure computational efficiency and gradient stability.
- **Visualization & Evaluation**:
  - `matplotlib` is used for real-time training progress visualization.
  - `opencv-python-headless` is recommended for server environments (e.g., AutoDL) to prevent `libGL` library errors while calculating Sobel gradients for the **Readability** metric.

#### Jittor Version

- **Role of Scipy**: Used in `RecLoss.py` to execute the **Hungarian Matcher**, which is the core matching logic of the DETR-like architecture.
- **OpenCV-Python-Headless**: Chosen to avoid `libGL.so` missing errors during `eval.py` debugging. For headless servers, this is best practice as it avoids installing large GUI system libraries.
- **Torch Usage**: Note that `torch` is **only** used in `convert_weight.py` to read the original `.pth` weights. The training and inference processes are 100% independent of PyTorch.

### ğŸ“‚Repository Structure

```
.
â”œâ”€â”€ pytorch_version/    # PyTorch implementation (Original Reproduction)
â”œâ”€â”€ jittor_version/     # Jittor implementation (Framework Migration)
â”œâ”€â”€ data/               # Dataset instructions and annotation files
â”œâ”€â”€ model_weight/       # Pretrained weights (Please download from Releases)
â””â”€â”€ assets/             # Images for README and documentation
```

### ğŸ¤Acknowledgements

- This project belongs to the **"Sprout Program" (æ–°èŠ½è®¡åˆ’)** of Nankai University.
- Special thanks to the teaching team of the **"AI Practice Course"** at the College of Artificial Intelligence, Nankai University, for their guidance.
- Thanks to the **Jittor** team for providing an excellent domestic deep learning framework.

### ğŸ“œCitations

```
@inproceedings{hsu2023posterlayout,
  title={PosterLayout: A New Benchmark and Approach for Content-aware Automatic Poster Layout},
  author={Hsu, Hsiao-Yuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}
```

## ä¸­æ–‡è¯´æ˜

### ğŸš€é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯å¯¹è®ºæ–‡ **"PosterLayout: A New Benchmark and Approach for Content-aware Automatic Poster Layout"** çš„å¤ç°å®ç°ã€‚

æœ¬ä»“åº“æ˜¯**å—å¼€å¤§å­¦**ã€Šäººå·¥æ™ºèƒ½å®è·µè¯¾ï¼ˆåˆçº§ï¼‰ã€‹è¯¾ç¨‹çš„å¤§ä½œä¸šæˆæœï¼Œå±äºå—å¼€å¤§å­¦**â€œæ–°èŠ½è®¡åˆ’â€**ã€‚ä¸»è¦å·¥ä½œåŒ…æ‹¬ï¼š
1. ä½¿ç”¨ **PyTorch** å¯¹åŸè®ºæ–‡è¿›è¡Œäº†åŸºå‡†å¤ç°ã€‚
2. å°†æ¨¡å‹å®Œæ•´è¿ç§»è‡³å›½äº§é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶ **Jittor (è®¡å›¾)**ã€‚

### ğŸ—ï¸æ¨¡å‹ç®€ä»‹ (PosterLayout / DS-GAN)

PosterLayout çš„æ ¸å¿ƒæ¨¡å‹ä¸º **DS-GAN**ï¼ˆè®¾è®¡åºåˆ—ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§æ„ŸçŸ¥å†…å®¹çš„è‡ªåŠ¨æµ·æŠ¥å¸ƒå±€ç”Ÿæˆæ¡†æ¶ï¼š
- **ç”Ÿæˆå™¨ (Generator)**ï¼šè¾“å…¥èƒŒæ™¯å›¾ä¸æ˜¾è‘—å›¾ã€‚é€šè¿‡è§†è§‰éª¨å¹²ç½‘ç»œæå–å›¾åƒç‰¹å¾ ($h_0$)ï¼Œç»“åˆ 1D-CNN å’ŒåŒå‘ LSTM (BiLSTM) å¯¹è®¾è®¡åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œæœ€ç»ˆé¢„æµ‹æ¯ä¸ªå…ƒç´ çš„ç±»åˆ« ($c_i$) å’Œåæ ‡ ($b_i$)ã€‚
- **åˆ¤åˆ«å™¨ (Discriminator)**ï¼šè¯„ä¼°ç”Ÿæˆçš„å¸ƒå±€åºåˆ—ä¸èƒŒæ™¯å›¾åƒçš„èåˆåˆç†æ€§ï¼Œåˆ¤æ–­å…¶ä¸ºçœŸ (Real) æˆ–å‡ (Fake)ã€‚
- **æŸå¤±å‡½æ•°**ï¼šç»“åˆäº†é‡æ„æŸå¤± ($L_{rec}$ï¼ŒåŒ…å«åˆ†ç±» NLLã€ä½ç½® L1 åŠ GIoU æŸå¤±) ä¸å¯¹æŠ—æŸå¤± ($L_{adv}$ï¼Œé‡‡ç”¨ Hinge Loss)ã€‚

<p align="center">
  <img src="./assets/framework.png" width="100%" />
  <br>
  <em>å›¾ï¼šDS-GAN æ¶æ„æ¦‚è§ˆ</em>
</p>

### ğŸ’¡è¿ç§»äº®ç‚¹ä¸æŠ€æœ¯ç»†èŠ‚ (Migration Highlights)

ä» PyTorch è¿ç§»è‡³ Jittor çš„è¿‡ç¨‹ä¸­ï¼Œé’ˆå¯¹æ¡†æ¶ç‰¹æ€§è¿›è¡Œäº†ä»¥ä¸‹ä¼˜åŒ–ï¼š

- **è‡ªå®šä¹‰ LSTM å±‚**ï¼šåœ¨ Jittor ä¸­æ‰‹åŠ¨å®ç°äº†åŒå‘ LSTM ç»“æ„ï¼Œç¡®ä¿èƒ½å¤Ÿå®Œç¾åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚
- **æ± åŒ–ç®—å­é€‚é…**ï¼šé’ˆå¯¹ Jittor æ± åŒ–å±‚å¯¹è¾“å…¥å°ºå¯¸çš„ä¸¥æ ¼æ ¡éªŒï¼Œé€šè¿‡ç»´åº¦æ‰©å……ï¼ˆWide-Concatï¼‰ç­–ç•¥ç»•è¿‡äº†å°ºå¯¸é™åˆ¶ã€‚
- **æ˜¾å­˜åŒæ­¥ä¼˜åŒ–**ï¼šåœ¨ GAN çš„åˆ¤åˆ«å™¨é€»è¾‘ä¸­ï¼Œåˆ©ç”¨ `jt.sync` ç²¾å‡†æ§åˆ¶åŒæ­¥æ—¶æœºï¼Œè§£å†³äº† ArgMax ç®—å­åœ¨ GPU ç¯å¢ƒä¸‹çš„æ•°æ®ä¸€è‡´æ€§é—®é¢˜ã€‚
- **è·¨æ¡†æ¶å¯¹é½**ï¼šé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ Loss æ˜ å°„ï¼Œç¡®ä¿äº† Jittor ç‰ˆæœ¬åœ¨æ”¶æ•›é€Ÿåº¦å’Œè¯„ä»·æŒ‡æ ‡ä¸Šä¸ PyTorch ç‰ˆæœ¬ä¿æŒä¸€è‡´ã€‚

### ğŸ¨æˆæœå±•ç¤º

æˆ‘å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬è§†è§‰ç¤ºä¾‹ã€å®šé‡æŒ‡æ ‡å¯¹æ¯”ï¼ˆä¸‰çº¿è¡¨æ ¼å¼ï¼‰ä»¥åŠä¸¤ä¸ªæ¡†æ¶çš„è®­ç»ƒæ”¶æ•›æ›²çº¿ã€‚æ¨¡å‹å±•ç°å‡ºäº†ä¼˜ç§€çš„è§†è§‰å†…å®¹æ„ŸçŸ¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆç¬¦åˆè§†è§‰å±‚çº§å¹¶é¿å¼€èƒŒæ™¯æ˜¾è‘—åŒºåŸŸçš„å¸ƒå±€ã€‚

#### è§†è§‰æ•ˆæœå¯¹æ¯”

<p align="center">
  <img src="./assets/pytorch_res1.png" width="45%" alt="PyTorch Sample 1"/>
  <img src="./assets/pytorch_res2.png" width="45%" alt="PyTorch Sample 2"/>
  <br><em>PyTorch Implementation Results</em>
</p>

<p align="center">
  <img src="./assets/jittor_res1.png" width="45%" alt="Jittor Sample 1"/>
  <img src="./assets/jittor_res2.png" width="45%" alt="Jittor Sample 2"/>
  <br><em>Jittor Implementation Results</em>
</p>

#### å®šé‡æŒ‡æ ‡å¯¹æ¯”

ä¸‹è¡¨æ±‡æ€»äº†æ¨¡å‹åœ¨ **ç¬¬ 300 è½® (Epoch 300)** çš„è¡¨ç°ã€‚ï¼ˆâ†‘ï¼‰è¡¨ç¤ºæ•°å€¼è¶Šé«˜è¶Šå¥½ï¼Œï¼ˆâ†“ï¼‰è¡¨ç¤ºè¶Šä½è¶Šå¥½ã€‚

<table>   <thead>     <tr>       <th>Method / Framework</th>       <th>Epoch</th>       <th>Val(â†‘)</th>       <th>Ove(â†“)</th>       <th>Ali(â†“)</th>       <th>Und_L(â†‘)</th>       <th>Und_S(â†‘)</th>       <th>Uti(â†‘)</th>       <th>Occ(â†“)</th>       <th>Rea(â†“)</th>     </tr>   </thead>   <tbody>     <tr>       <td><b>(Ours) PyTorch</b></td>       <td>300</td>       <td>1.0000</td>       <td>0.0137</td>       <td><b>0.0039</b></td>       <td><b>0.4184</b></td>       <td>0.2082</td>       <td><b>0.9786</b></td>       <td>0.3468</td>       <td><b>0.0121</b></td>     </tr>     <tr>       <td><b>(Ours) Jittor</b></td>       <td>300</td>       <td>1.0000</td>       <td><b>0.0076</b></td>       <td>0.0045</td>       <td>0.2380</td>       <td><b>0.4999</b></td>       <td>0.7371</td>       <td><b>0.1763</b></td>       <td>0.1460</td>     </tr>   </tbody> </table>

#### è®­ç»ƒæ”¶æ•›è¶‹åŠ¿

ä¸ºäº†å±•ç¤ºå¤ç°çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬è®°å½•å¹¶æä¾›äº† PyTorch å’Œ Jittor ç‰ˆæœ¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡å˜åŒ–æ›²çº¿ã€‚

<p align="center">   <img src="./assets/pytorch_metrics.png" width="80%" />   <br><em>Metrics Trend: PyTorch Version</em> </p>

<p align="center">   <img src="./assets/jittor_metrics.png" width="80%" />   <br><em>Metrics Trend: Jittor Version</em> </p>

### ğŸ”ç»“æœåˆ†æ

- **æ¡†æ¶ä¸€è‡´æ€§**ï¼šä¸¤ä¸ªæ¡†æ¶å‡è¾¾åˆ°äº† **1.0 çš„ Validityï¼ˆæœ‰æ•ˆæ€§ï¼‰**ï¼Œè¯´æ˜ç”Ÿæˆçš„å¸ƒå±€å®Œå…¨ç¬¦åˆåŸºç¡€å‡ ä½•è§„èŒƒã€‚
- **æŒ‡æ ‡äº®ç‚¹**ï¼š
  - **PyTorch ç‰ˆæœ¬**åœ¨ **Utility (0.9786)** å’Œ **Readability (0.0121)** è¡¨ç°æ›´ä¼˜ï¼Œè¯´æ˜å…¶åœ¨é¿è®©è§†è§‰ä¸­å¿ƒå’Œä¿æŒæ–‡å­—æ¸…æ™°åº¦æ–¹é¢æœ‰æ›´å¼ºçš„èƒ½åŠ›ã€‚
  - **Jittor ç‰ˆæœ¬**åœ¨ **Overlap (0.0076)** å’Œ **Underlay_S (0.4999)** æŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½ï¼Œè¿™è¡¨æ˜ Jittor å®ç°åœ¨æ§åˆ¶å…ƒç´ é‡å ä»¥åŠå¤„ç†åº•å—ï¼ˆUnderlayï¼‰åŒ…å«å…³ç³»æ–¹é¢æ›´ä¸ºå‡ºè‰²ã€‚

- **æ”¶æ•›æ€§**ï¼šå¦‚è¶‹åŠ¿å›¾æ‰€ç¤ºï¼Œä¸¤ä¸ªç‰ˆæœ¬åœ¨ 300 è½®è®­ç»ƒä¸­å„é¡¹æŒ‡æ ‡å‡å‘ˆç°ç¨³å®šçš„æ”¶æ•›è¶‹åŠ¿ï¼ŒéªŒè¯äº†å®éªŒå‚æ•°è®¾ç½®ä¸è¿ç§»é€»è¾‘çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“–ä½¿ç”¨è¯´æ˜ (ä¸­æ–‡)

1. **æ•°æ®å‡†å¤‡**: è¯·å‚è€ƒ [data/README.md](https://www.google.com/url?sa=E&q=./data/README.md) ä¸‹è½½ **PosterLLaVa** æ•°æ®é›†ï¼Œå¹¶ç¡®ä¿å›¾åƒæ–‡ä»¶å¤¹ä¸ JSON æ ‡æ³¨æ–‡ä»¶æ­£ç¡®æ”¾ç½®åœ¨ `data/` ç›®å½•ä¸‹ã€‚

2. **æƒé‡ä¸‹è½½**: ä» [Releases](https://www.google.com/url?sa=E&q=../../releases) ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼ˆPyTorch æ ¼å¼ä¸º `.pth`ï¼ŒJittor æ ¼å¼ä¸º `.jtp`ï¼‰ï¼Œå¹¶æ”¾å…¥ `model_weight/` æ–‡ä»¶å¤¹ã€‚

3. **è®­ç»ƒä¸æ¨ç†**:
   è¿›å…¥å¯¹åº”çš„æ¡†æ¶ç›®å½•æ‰§è¡Œè„šæœ¬ï¼š

   ```
   cd jittor_version  # æˆ– cd pytorch_version
   python main.py     # å¯åŠ¨è®­ç»ƒ
   python infer.py    # æ‰¹é‡æ¨ç†ç”Ÿæˆå¸ƒå±€
   python eval.py     # è®¡ç®—è¯„ä»·æŒ‡æ ‡å¹¶è¿›è¡Œå¯è§†åŒ–ä¿å­˜
   ```

### ğŸ› ï¸å®‰è£…ä¸ç¯å¢ƒé…ç½®

#### Pytorch ç‰ˆæœ¬

å®‰è£… Python ä¾èµ–ï¼š

```
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

#### Jittor ç‰ˆæœ¬

1. **å®‰è£… Python ä¾èµ–**ï¼š

   ```
   python -m pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
   ```

2. **åˆå§‹åŒ– Jittor CUDA ç¯å¢ƒ** (é’ˆå¯¹ AutoDL æˆ– GPU æœåŠ¡å™¨)ï¼š
   Jittor æ¶‰åŠ C++ ä»£ç ç¼–è¯‘ï¼Œå¿…é¡»é…ç½®å¼€å‘ç¯å¢ƒã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è®© Jittor è‡ªåŠ¨ä¸‹è½½å¹¶é…ç½®å†…ç½®çš„ CUDA å’Œ cuDNNï¼š

   ```
   python -m jittor_utils.install_cuda
   ```

3. **ä¿®å¤ç³»ç»Ÿè·¯å¾„** (å¤„ç† `libGL` æˆ–å¤´æ–‡ä»¶ç¼ºå¤±)ï¼š
   å¦‚æœä½¿ç”¨çš„æ˜¯è€ç‰ˆæœ¬é•œåƒï¼Œæˆ–é‡åˆ° `cudnn.h not found`ï¼Œè¯·æ‰§è¡Œï¼š

   ```
   # 1. å°è¯•å»ºç«‹è½¯é“¾æ¥ (éœ€è¦ sudo)
   sudo ln -s /usr/include/cudnn.h /usr/local/cuda/include/cudnn.h 2>/dev/null
   
   # 2. å¦‚æœæ²¡æœ‰ sudo æƒé™ï¼Œé…ç½®ç¯å¢ƒå˜é‡
   export C_INCLUDE_PATH=$C_INCLUDE_PATH:/usr/include
   export CPLUS_INCLUDE_PATH=$CPLUS_INCLUDE_PATH:/usr/include
   # è§£å†³ OpenMP ä¸åº•å±‚ MKL å†²çª
   export KMP_DUPLICATE_LIB_OK=TRUE
   export MKL_SERVICE_FORCE_INTEL=1
   ```

### ğŸ““æŠ€æœ¯è¯´æ˜

#### Pytorch ç‰ˆæœ¬

- **éª¨å¹²ç½‘ç»œåº“**ï¼šæœ¬é¡¹ç›®ä½¿ç”¨ `timm` (PyTorch Image Models) åº“æ¥åˆå§‹åŒ– ResNet éª¨å¹²ç½‘ç»œã€‚ä»£ç æ”¯æŒä»æœ¬åœ°è·¯å¾„åŠ è½½æƒé‡ï¼Œä¹Ÿæ”¯æŒåœ¨çº¿ä¸‹è½½å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ã€‚
- **åŒˆç‰™åˆ©åŒ¹é…**ï¼šå‚è€ƒ DETR æ¶æ„ï¼Œæˆ‘ä»¬åœ¨ `RecLoss.py` ä¸­è°ƒç”¨äº† `scipy.optimize.linear_sum_assignment` æ¥æ±‚è§£é¢„æµ‹æ¡†ä¸çœŸå®æ¡†ä¹‹é—´çš„äºŒåˆ†å›¾åŒ¹é…é—®é¢˜ã€‚
- **å‡ ä½•ç®—å­**ï¼šå¤æ‚çš„è¾¹ç•Œæ¡†è®¡ç®—ï¼ˆå¦‚ `box_area`ï¼‰é€šè¿‡ `torchvision.ops` å®ç°ï¼Œä»¥ç¡®ä¿è®¡ç®—æ•ˆç‡å’Œæ¢¯åº¦çš„ç¨³å®šæ€§ã€‚
- **å¯è§†åŒ–ä¸è¯„ä¼°**ï¼š
  - ä½¿ç”¨ `matplotlib` åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå®æ—¶çš„å¸ƒå±€æ•ˆæœå›¾å’ŒæŒ‡æ ‡æ›²çº¿ã€‚
  - åœ¨æœåŠ¡å™¨ç¯å¢ƒï¼ˆå¦‚ AutoDLï¼‰ä¸­ï¼Œå»ºè®®ä½¿ç”¨ `opencv-python-headless`ã€‚è¿™å¯ä»¥é¿å…åœ¨è®¡ç®— **Readabilityï¼ˆå¯è¯»æ€§ï¼‰** æŒ‡æ ‡æ‰€éœ€çš„ Sobel æ¢¯åº¦æ—¶ï¼Œå› ç¼ºå°‘ç³»ç»Ÿå›¾å½¢åº“è€Œè§¦å‘çš„ `libGL` æŠ¥é”™ã€‚

#### Jittor ç‰ˆæœ¬

- **Scipy çš„ä½œç”¨**ï¼šåœ¨ `RecLoss.py` ä¸­ç”¨äºæ‰§è¡Œ**åŒˆç‰™åˆ©åŒ¹é…ç®—æ³•** (Hungarian Matcher)ï¼Œæ˜¯è¯¥æ¶æ„ä¸­æ ¸å¿ƒçš„åŒ¹é…é€»è¾‘ã€‚
- **OpenCV-Python-Headless**ï¼šæˆ‘åœ¨è°ƒè¯• `eval.py` æ—¶å‘ç° `import cv2` ä¼šæŠ¥ `libGL.so` ç¼ºå¤±ã€‚å¯¹äºæ²¡æœ‰æ˜¾ç¤ºå™¨çš„æœåŠ¡å™¨ï¼Œå®‰è£… Headless ç‰ˆæœ¬æ˜¯æœ€ä½³å®è·µï¼Œèƒ½çœå»å®‰è£…å‡ ç™¾ MB ç³»ç»Ÿå›¾å½¢åº“çš„éº»çƒ¦ã€‚
- **PyTorch çš„è§’è‰²**ï¼šæ˜ç¡®è¯´æ˜ `torch` **ä»…**åœ¨ `convert_weight.py` è„šæœ¬ä¸­ç”¨äºè¯»å–åŸå§‹ `.pth` æƒé‡ï¼Œè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹å®Œå…¨ä¸ä¾èµ– PyTorchã€‚

###  ğŸ“‚ä»“åº“ç»“æ„

```text
.
â”œâ”€â”€ pytorch_version/    # PyTorch implementation
â”œâ”€â”€ jittor_version/     # Jittor implementation
â”œâ”€â”€ data/               # Dataset instructions and annotation files
â”œâ”€â”€ model_weight/       # Placeholder for model checkpoints
â””â”€â”€ assets/             # Images for README and documentation
```

### ğŸ¤è‡´è°¢

- æœ¬é¡¹ç›®å±äºå—å¼€å¤§å­¦**â€œæ–°èŠ½è®¡åˆ’â€**é¡¹ç›®æˆæœã€‚
- æ„Ÿè°¢å—å¼€å¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢ã€Šäººå·¥æ™ºèƒ½å®è·µè¯¾ã€‹æ•™å­¦å›¢é˜Ÿåœ¨å¤ç°è¿‡ç¨‹ä¸­çš„æ‚‰å¿ƒæŒ‡å¯¼ã€‚
- æ„Ÿè°¢ **Jittor (è®¡å›¾)** å›¢é˜Ÿæä¾›äº†ä¼˜ç§€çš„å›½äº§æ·±åº¦å­¦ä¹ æ¡†æ¶æ”¯æŒã€‚

### ğŸ“œå¼•ç”¨

```
@inproceedings{hsu2023posterlayout,
  title={PosterLayout: A New Benchmark and Approach for Content-aware Automatic Poster Layout},
  author={Hsu, Hsiao-Yuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

```
